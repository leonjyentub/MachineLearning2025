{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from https://medium.com/@thisislong/building-a-recurrent-neural-network-from-scratch-ba9b27a42856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "def string_to_one_hot(inputs: np.ndarray) -> np.ndarray:\n",
    "    char_to_index = {char: i for i, char in enumerate(string.ascii_uppercase)}\n",
    "\n",
    "    one_hot_inputs = []\n",
    "    for row in inputs:\n",
    "        one_hot_list = []\n",
    "        for char in row:\n",
    "            if char.upper() in char_to_index:\n",
    "                one_hot_vector = np.zeros((len(string.ascii_uppercase), 1))\n",
    "                one_hot_vector[char_to_index[char.upper()]] = 1\n",
    "                one_hot_list.append(one_hot_vector)\n",
    "        one_hot_inputs.append(one_hot_list)\n",
    "\n",
    "    return np.array(one_hot_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    ##return np.exp(x-np.max(x)) / np.sum(np.exp(x), axis=0)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer:\n",
    "    inputs: np.ndarray\n",
    "    U: np.ndarray = None\n",
    "    delta_U: np.ndarray = None\n",
    "\n",
    "    def __init__(self, inputs: np.ndarray, hidden_size: int):\n",
    "        self.inputs = inputs\n",
    "        self.U = np.random.uniform(low=0, high=1, size=(hidden_size, len(inputs[0])))\n",
    "        self.delta_U = np.zeros_like(self.U)\n",
    "\n",
    "    def get_input(self, time_step: int) -> np.ndarray:\n",
    "        return self.inputs[time_step]\n",
    "\n",
    "    def weighted_sum(self, time_step: int) -> np.ndarray:\n",
    "        return self.U @ self.get_input(time_step)\n",
    "\n",
    "    def calculate_deltas_per_step(self, time_step: int, delta_weighted_sum: np.ndarray):\n",
    "        # (h_dimension, 1) @ (1, input_size) = (h_dimension, input_size)\n",
    "        self.delta_U += delta_weighted_sum @ self.get_input(time_step).T\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        self.U -= learning_rate * self.delta_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "    states: np.ndarray = None\n",
    "    W: np.ndarray = None\n",
    "    delta_W: np.ndarray = None\n",
    "    bias: np.ndarray = None\n",
    "    delta_bias: np.ndarray = None\n",
    "    next_delta_activation: np.ndarray = None\n",
    "\n",
    "    def __init__(self, vocab_size: int, size: int):\n",
    "        self.W = np.random.uniform(low=0, high=1, size=(size, size))\n",
    "        self.bias = np.random.uniform(low=0, high=1, size=(size, 1))\n",
    "        self.states = np.zeros(shape=(vocab_size, size, 1))\n",
    "        self.next_delta_activation = np.zeros(shape=(size, 1))\n",
    "        self.delta_bias = np.zeros_like(self.bias)\n",
    "        self.delta_W = np.zeros_like(self.W)\n",
    "\n",
    "    def get_hidden_state(self, time_step: int) -> np.ndarray:\n",
    "        # If starting out at the beginning of the sequence, a[t-1] will return zeros\n",
    "        if time_step < 0:\n",
    "            return np.zeros_like(self.states[0])\n",
    "        return self.states[time_step]\n",
    "\n",
    "    def set_hidden_state(self, time_step: int, hidden_state: np.ndarray):\n",
    "        self.states[time_step] = hidden_state\n",
    "\n",
    "    def activate(self, weighted_input: np.ndarray, time_step: int) -> np.ndarray:\n",
    "        previous_hidden_state = self.get_hidden_state(time_step - 1)\n",
    "        # W @ h_prev => (h_dimension, h_dimension) @ (h_dimension, 1) = (h_dimension, 1)\n",
    "        weighted_hidden_state = self.W @ previous_hidden_state\n",
    "        # (h_dimension, 1) + (h_dimension, 1) + (h_dimension, 1) = (h_dimension, 1)\n",
    "        weighted_sum = weighted_input + weighted_hidden_state + self.bias\n",
    "        activation = np.tanh(weighted_sum)  # (h_dimension, 1)\n",
    "        self.set_hidden_state(time_step, activation)\n",
    "        return activation\n",
    "\n",
    "    def calculate_deltas_per_step(self, time_step: int, delta_output: np.ndarray) -> np.ndarray:\n",
    "        # (h_dimension, 1) + (h_dimension, 1) = (h_dimension, 1)\n",
    "        delta_activation = delta_output + self.next_delta_activation\n",
    "        # (h_dimension, 1) * scalar = (h_dimension, 1)\n",
    "        delta_weighted_sum = delta_activation * (\n",
    "            1 - self.get_hidden_state(time_step) ** 2\n",
    "        )\n",
    "        # (h_dimension, h_dimension) @ (h_dimension, 1) = (h_dimension, 1)\n",
    "        self.next_delta_activation = self.W.T @ delta_weighted_sum\n",
    "\n",
    "        # (h_dimension, 1) @ (1, h_dimension) = (h_dimension, h_dimension)\n",
    "        self.delta_W += delta_weighted_sum @ self.get_hidden_state(time_step - 1).T\n",
    "\n",
    "        # derivative of hidden bias is the same as dL_ds\n",
    "        self.delta_bias += delta_weighted_sum\n",
    "        return delta_weighted_sum\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        self.W -= learning_rate * self.delta_W\n",
    "        self.bias -= learning_rate * self.delta_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer:\n",
    "    states: np.ndarray = None\n",
    "    V: np.ndarray = None\n",
    "    bias: np.ndarray = None\n",
    "    delta_bias: np.ndarray = None\n",
    "    delta_V: np.ndarray = None\n",
    "\n",
    "    def __init__(self, size: int, hidden_size: int):\n",
    "        self.V = np.random.uniform(low=0, high=1, size=(size, hidden_size))\n",
    "        self.bias = np.random.uniform(low=0, high=1, size=(size, 1))\n",
    "        self.states = np.zeros(shape=(size, size, 1))\n",
    "        self.delta_bias = np.zeros_like(self.bias)\n",
    "        self.delta_V = np.zeros_like(self.V)\n",
    "\n",
    "    def predict(self, hidden_state: np.ndarray, time_step: int) -> np.ndarray:\n",
    "        # V @ h => (input_size, h_dimension) @ (h_dimension, 1) = (input_size, 1)\n",
    "        # (input_size, 1) + (input_size, 1) = (input_size, 1)\n",
    "        output = self.V @ hidden_state + self.bias\n",
    "        prediction = softmax(output)\n",
    "        self.set_state(time_step, prediction)\n",
    "        return prediction\n",
    "\n",
    "    def get_state(self, time_step: int) -> np.ndarray:\n",
    "        return self.states[time_step]\n",
    "\n",
    "    def set_state(self, time_step: int, prediction: np.ndarray):\n",
    "        self.states[time_step] = prediction\n",
    "\n",
    "    def calculate_deltas_per_step(\n",
    "        self,\n",
    "        expected: np.ndarray,\n",
    "        hidden_state: np.ndarray,\n",
    "        time_step: int,\n",
    "    ) -> np.ndarray:\n",
    "        # dL_do = dL_dyhat * dyhat_do = derivative of loss function * derivative of softmax\n",
    "        # dL_do = step.y_hat - expected[step_number]\n",
    "        delta_output = self.get_state(time_step) - expected  # (input_size, 1)\n",
    "\n",
    "        # (input_size, 1) @ (1, hidden_size) = (input_size, hidden_size)\n",
    "        self.delta_V += delta_output @ hidden_state.T\n",
    "\n",
    "        # dL_dc += dL_do\n",
    "        self.delta_bias += delta_output\n",
    "        return self.V.T @ delta_output\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float):\n",
    "        self.V -= learning_rate * self.delta_V\n",
    "        self.bias -= learning_rate * self.delta_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    hidden_layer: HiddenLayer\n",
    "    output_layer: OutputLayer\n",
    "    alpha: float  # learning rate\n",
    "    input_layer: InputLayer = None\n",
    "\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, alpha: float) -> None:\n",
    "        self.hidden_layer = HiddenLayer(vocab_size, hidden_size)\n",
    "        self.output_layer = OutputLayer(vocab_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def feed_forward(self, inputs: np.ndarray) -> OutputLayer:\n",
    "        self.input_layer = InputLayer(inputs, self.hidden_size)\n",
    "        for step in range(len(inputs)):\n",
    "            weighted_input = self.input_layer.weighted_sum(step)\n",
    "            activation = self.hidden_layer.activate(weighted_input, step)\n",
    "            self.output_layer.predict(activation, step)\n",
    "        return self.output_layer\n",
    "\n",
    "    def backpropagation(self, expected: np.ndarray) -> None:\n",
    "        for step_number in reversed(range(len(expected))):\n",
    "            delta_output = self.output_layer.calculate_deltas_per_step(\n",
    "                expected[step_number],\n",
    "                self.hidden_layer.get_hidden_state(step_number),\n",
    "                step_number,\n",
    "            )\n",
    "            delta_weighted_sum = self.hidden_layer.calculate_deltas_per_step(\n",
    "                step_number, delta_output\n",
    "            )\n",
    "            self.input_layer.calculate_deltas_per_step(step_number, delta_weighted_sum)\n",
    "\n",
    "        self.output_layer.update_weights_and_bias(self.alpha)\n",
    "        self.hidden_layer.update_weights_and_bias(self.alpha)\n",
    "        self.input_layer.update_weights_and_bias(self.alpha)\n",
    "\n",
    "    def loss(self, y_hat: list[np.ndarray], y: list[np.ndarray]) -> float:\n",
    "        \"\"\"\n",
    "        Cross-entropy loss function - Calculating difference between 2 probability distributions.\n",
    "        First, calculate cross-entropy loss for each time step with np.sum, which returns a numpy array\n",
    "        Then, sum across individual losses of all time steps with sum() to get a scalar value.\n",
    "        :param y_hat: predicted value\n",
    "        :param y: expected value - true label\n",
    "        :return: total loss\n",
    "        \"\"\"\n",
    "        return sum(-np.sum(y[i] * np.log(y_hat[i]) for i in range(len(y))))\n",
    "\n",
    "    def train(self, inputs: np.ndarray, expected: np.ndarray, epochs: int) -> None:\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"epoch={epoch}\")\n",
    "            for idx, input in enumerate(inputs):\n",
    "                y_hats = self.feed_forward(input)\n",
    "                self.backpropagation(expected[idx])\n",
    "                print(f\"Loss round: {self.loss([y for y in y_hats.states], expected[idx])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n",
      "Loss round: [149.65079379]\n",
      "Loss round: [147.27678994]\n",
      "Loss round: [141.46285952]\n",
      "Loss round: [134.9030155]\n",
      "Loss round: [127.01415922]\n",
      "epoch=1\n",
      "Loss round: [120.82100671]\n",
      "Loss round: [119.93052235]\n",
      "Loss round: [117.82055006]\n",
      "Loss round: [115.93575959]\n",
      "Loss round: [113.41611135]\n",
      "epoch=2\n",
      "Loss round: [110.26043536]\n",
      "Loss round: [110.05788575]\n",
      "Loss round: [108.03767907]\n",
      "Loss round: [106.17580362]\n",
      "Loss round: [105.35104677]\n",
      "epoch=3\n",
      "Loss round: [103.61704]\n",
      "Loss round: [103.77706573]\n",
      "Loss round: [103.57102653]\n",
      "Loss round: [101.28661898]\n",
      "Loss round: [102.83306788]\n",
      "epoch=4\n",
      "Loss round: [102.54327992]\n",
      "Loss round: [102.85358983]\n",
      "Loss round: [105.59964652]\n",
      "Loss round: [103.60921791]\n",
      "Loss round: [106.99631634]\n",
      "epoch=5\n",
      "Loss round: [107.78782122]\n",
      "Loss round: [109.65072533]\n",
      "Loss round: [113.16210147]\n",
      "Loss round: [111.71045954]\n",
      "Loss round: [116.36578971]\n",
      "epoch=6\n",
      "Loss round: [119.26202528]\n",
      "Loss round: [120.63739082]\n",
      "Loss round: [124.60043151]\n",
      "Loss round: [121.34980688]\n",
      "Loss round: [128.53325579]\n",
      "epoch=7\n",
      "Loss round: [131.70846546]\n",
      "Loss round: [132.78481481]\n",
      "Loss round: [136.37440633]\n",
      "Loss round: [131.12392303]\n",
      "Loss round: [138.88900045]\n",
      "epoch=8\n",
      "Loss round: [140.31447609]\n",
      "Loss round: [138.66419432]\n",
      "Loss round: [141.29291374]\n",
      "Loss round: [133.58453968]\n",
      "Loss round: [139.74180449]\n",
      "epoch=9\n",
      "Loss round: [140.73943048]\n",
      "Loss round: [135.33666497]\n",
      "Loss round: [135.13687229]\n",
      "Loss round: [124.25929625]\n",
      "Loss round: [127.19430216]\n",
      "1\n",
      "B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonj\\AppData\\Local\\Temp\\ipykernel_32332\\446663555.py:46: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(-np.sum(y[i] * np.log(y_hat[i]) for i in range(len(y))))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  inputs = np.array([\n",
    "      [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "      [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
    "      [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
    "      [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
    "      [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
    "  ])\n",
    "\n",
    "  expected = np.array([\n",
    "      [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
    "      [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "      [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"], \n",
    "      [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
    "      [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "  ])\n",
    "  \n",
    "  one_hot_inputs = string_to_one_hot(inputs)\n",
    "  one_hot_expected = string_to_one_hot(expected)\n",
    "\n",
    "  # Forward pass through time, no gradient clipping yet so there will be gradient exploding problem\n",
    "  # https://stackoverflow.com/a/33980220\n",
    "  # https://stackoverflow.com/a/72494516\n",
    "  rnn = VanillaRNN(vocab_size=len(string.ascii_uppercase), hidden_size=128, alpha=0.0001)\n",
    "  rnn.train(one_hot_inputs, one_hot_expected, epochs=10)\n",
    "\n",
    "  new_inputs = np.array([[\"B\", \"C\", \"D\"]])\n",
    "  for input in string_to_one_hot(new_inputs):\n",
    "      predictions = rnn.feed_forward(input)\n",
    "      output = np.argmax(predictions.states[-1])\n",
    "      print(output) # index of the one-hot value of prediction\n",
    "      print(string.ascii_uppercase[output]) # mapping one hot to character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2ML0328",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
